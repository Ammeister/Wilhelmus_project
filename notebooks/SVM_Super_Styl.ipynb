{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12a345df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.svm as sk\n",
    "import sklearn.metrics as metrics\n",
    "import sklearn.decomposition as decomp\n",
    "import sklearn.preprocessing as preproc\n",
    "import sklearn.pipeline as skp\n",
    "import sklearn.model_selection as skmodel\n",
    "import pandas as pd\n",
    "import minisom\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "726fe302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'features_train_auteurs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f166c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.rename(columns={'Unnamed: 0': 'index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e398cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.set_index(\"index\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e3c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44d27b6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_v_pron</th>\n",
       "      <th>pron_n_v</th>\n",
       "      <th>adp_pron_n</th>\n",
       "      <th>adp_art_n</th>\n",
       "      <th>v_pron_n</th>\n",
       "      <th>n_adv_v</th>\n",
       "      <th>n_adp_n</th>\n",
       "      <th>art_n_v</th>\n",
       "      <th>adv_v_pron</th>\n",
       "      <th>v_pron_v</th>\n",
       "      <th>...</th>\n",
       "      <th>geen</th>\n",
       "      <th>om</th>\n",
       "      <th>nu</th>\n",
       "      <th>wel</th>\n",
       "      <th>ons</th>\n",
       "      <th>haten</th>\n",
       "      <th>deze</th>\n",
       "      <th>wat</th>\n",
       "      <th>dit</th>\n",
       "      <th>bij</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.016949</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.021186</td>\n",
       "      <td>0.004237</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.008475</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Jacobus de Ruyter</th>\n",
       "      <td>0.014388</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.014388</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.006579</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mattheus Gargon</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.032787</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.041379</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.027586</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.009950</td>\n",
       "      <td>0.007463</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>0.002488</td>\n",
       "      <td>0.004975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.027273</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.022727</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033613</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008403</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034884</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027027</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.013158</td>\n",
       "      <td>0.008772</td>\n",
       "      <td>0.004386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.015504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.021053</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.015789</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.010526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.024194</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22209 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   n_v_pron  pron_n_v  adp_pron_n  adp_art_n  v_pron_n  \\\n",
       "index                                                                    \n",
       "Anonymous          0.016949  0.008475    0.012712   0.021186  0.004237   \n",
       "Jacobus de Ruyter  0.014388  0.017986    0.028777   0.007194  0.007194   \n",
       "Mattheus Gargon    0.000000  0.000000    0.016393   0.000000  0.000000   \n",
       "Anonymous          0.041379  0.013793    0.013793   0.034483  0.027586   \n",
       "Anonymous          0.004975  0.000000    0.007463   0.009950  0.007463   \n",
       "...                     ...       ...         ...        ...       ...   \n",
       "Anonymous          0.027273  0.027273    0.009091   0.004545  0.013636   \n",
       "Anonymous          0.011628  0.011628    0.011628   0.000000  0.011628   \n",
       "Anonymous          0.008772  0.021930    0.021930   0.013158  0.008772   \n",
       "Anonymous          0.021053  0.010526    0.010526   0.015789  0.015789   \n",
       "Anonymous          0.024194  0.000000    0.032258   0.008065  0.032258   \n",
       "\n",
       "                    n_adv_v   n_adp_n   art_n_v  adv_v_pron  v_pron_v  ...  \\\n",
       "index                                                                  ...   \n",
       "Anonymous          0.012712  0.008475  0.008475    0.012712  0.000000  ...   \n",
       "Jacobus de Ruyter  0.017986  0.003597  0.003597    0.017986  0.014388  ...   \n",
       "Mattheus Gargon    0.016393  0.032787  0.000000    0.000000  0.016393  ...   \n",
       "Anonymous          0.006897  0.000000  0.006897    0.006897  0.006897  ...   \n",
       "Anonymous          0.002488  0.004975  0.004975    0.002488  0.004975  ...   \n",
       "...                     ...       ...       ...         ...       ...  ...   \n",
       "Anonymous          0.004545  0.000000  0.004545    0.009091  0.022727  ...   \n",
       "Anonymous          0.000000  0.034884  0.000000    0.000000  0.000000  ...   \n",
       "Anonymous          0.000000  0.008772  0.013158    0.008772  0.004386  ...   \n",
       "Anonymous          0.005263  0.005263  0.015789    0.005263  0.010526  ...   \n",
       "Anonymous          0.008065  0.008065  0.008065    0.016129  0.016129  ...   \n",
       "\n",
       "                       geen        om        nu       wel       ons     haten  \\\n",
       "index                                                                           \n",
       "Anonymous          0.008547  0.000000  0.000000  0.008547  0.000000  0.000000   \n",
       "Jacobus de Ruyter  0.000000  0.006579  0.006579  0.006579  0.000000  0.000000   \n",
       "Mattheus Gargon    0.000000  0.037037  0.000000  0.000000  0.000000  0.037037   \n",
       "Anonymous          0.013889  0.000000  0.000000  0.000000  0.013889  0.000000   \n",
       "Anonymous          0.000000  0.000000  0.005051  0.005051  0.000000  0.000000   \n",
       "...                     ...       ...       ...       ...       ...       ...   \n",
       "Anonymous          0.033613  0.008403  0.008403  0.008403  0.000000  0.008403   \n",
       "Anonymous          0.000000  0.000000  0.027027  0.000000  0.000000  0.027027   \n",
       "Anonymous          0.015504  0.015504  0.000000  0.007752  0.000000  0.007752   \n",
       "Anonymous          0.000000  0.000000  0.000000  0.030928  0.000000  0.000000   \n",
       "Anonymous          0.027778  0.027778  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "                       deze       wat       dit       bij  \n",
       "index                                                      \n",
       "Anonymous          0.025641  0.008547  0.000000  0.008547  \n",
       "Jacobus de Ruyter  0.000000  0.000000  0.000000  0.000000  \n",
       "Mattheus Gargon    0.000000  0.000000  0.074074  0.000000  \n",
       "Anonymous          0.000000  0.000000  0.027778  0.000000  \n",
       "Anonymous          0.005051  0.010101  0.000000  0.010101  \n",
       "...                     ...       ...       ...       ...  \n",
       "Anonymous          0.000000  0.000000  0.000000  0.008403  \n",
       "Anonymous          0.000000  0.000000  0.000000  0.000000  \n",
       "Anonymous          0.000000  0.000000  0.007752  0.000000  \n",
       "Anonymous          0.010309  0.000000  0.020619  0.000000  \n",
       "Anonymous          0.000000  0.000000  0.000000  0.013889  \n",
       "\n",
       "[22209 rows x 148 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1ee042eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(r'features_test_auteurs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3796f6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.rename(columns={'Unnamed: 0': 'index'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35cd61b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.set_index(\"index\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6838c958",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b0730c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_v_pron</th>\n",
       "      <th>pron_n_v</th>\n",
       "      <th>adp_pron_n</th>\n",
       "      <th>adp_art_n</th>\n",
       "      <th>v_pron_n</th>\n",
       "      <th>n_adv_v</th>\n",
       "      <th>n_adp_n</th>\n",
       "      <th>art_n_v</th>\n",
       "      <th>adv_v_pron</th>\n",
       "      <th>v_pron_v</th>\n",
       "      <th>...</th>\n",
       "      <th>geen</th>\n",
       "      <th>om</th>\n",
       "      <th>nu</th>\n",
       "      <th>wel</th>\n",
       "      <th>ons</th>\n",
       "      <th>haten</th>\n",
       "      <th>deze</th>\n",
       "      <th>wat</th>\n",
       "      <th>dit</th>\n",
       "      <th>bij</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.022222</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.007737</td>\n",
       "      <td>0.023211</td>\n",
       "      <td>0.023211</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005803</td>\n",
       "      <td>0.025145</td>\n",
       "      <td>0.025145</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.004032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.017621</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>0.004405</td>\n",
       "      <td>0.013216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.036036</td>\n",
       "      <td>0.009009</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>0.018018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.053571</td>\n",
       "      <td>0.017857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>0.011952</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.003984</td>\n",
       "      <td>0.011952</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.019920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.028226</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.014113</td>\n",
       "      <td>0.014113</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>0.010081</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>0.020161</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>0.004065</td>\n",
       "      <td>0.024390</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.016260</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.010753</td>\n",
       "      <td>0.016129</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.008065</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.027473</td>\n",
       "      <td>0.010989</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>0.005495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.011062</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.015487</td>\n",
       "      <td>0.022124</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.006637</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.013793</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Anonymous</th>\n",
       "      <td>0.051672</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.036474</td>\n",
       "      <td>0.009119</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>0.006079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051948</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5502 rows × 148 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           n_v_pron  pron_n_v  adp_pron_n  adp_art_n  v_pron_n   n_adv_v  \\\n",
       "index                                                                      \n",
       "Anonymous  0.014815  0.007407    0.007407   0.022222  0.000000  0.007407   \n",
       "Anonymous  0.003868  0.017408    0.007737   0.007737  0.023211  0.023211   \n",
       "Anonymous  0.017621  0.004405    0.000000   0.000000  0.004405  0.008811   \n",
       "Anonymous  0.018018  0.000000    0.018018   0.009009  0.000000  0.000000   \n",
       "Anonymous  0.015936  0.007968    0.011952   0.019920  0.007968  0.003984   \n",
       "...             ...       ...         ...        ...       ...       ...   \n",
       "Anonymous  0.028226  0.016129    0.014113   0.014113  0.022177  0.012097   \n",
       "Anonymous  0.018817  0.013441    0.016129   0.016129  0.010753  0.016129   \n",
       "Anonymous  0.011062  0.017699    0.017699   0.015487  0.022124  0.008850   \n",
       "Anonymous  0.006897  0.013793    0.000000   0.013793  0.013793  0.006897   \n",
       "Anonymous  0.051672  0.006079    0.036474   0.006079  0.036474  0.009119   \n",
       "\n",
       "            n_adp_n   art_n_v  adv_v_pron  v_pron_v  ...      geen        om  \\\n",
       "index                                                ...                       \n",
       "Anonymous  0.014815  0.000000    0.000000  0.007407  ...  0.014706  0.000000   \n",
       "Anonymous  0.000000  0.005803    0.025145  0.025145  ...  0.004032  0.004032   \n",
       "Anonymous  0.004405  0.013216    0.000000  0.008811  ...  0.000000  0.000000   \n",
       "Anonymous  0.036036  0.009009    0.018018  0.018018  ...  0.017857  0.000000   \n",
       "Anonymous  0.003984  0.011952    0.015936  0.019920  ...  0.007752  0.000000   \n",
       "...             ...       ...         ...       ...  ...       ...       ...   \n",
       "Anonymous  0.010081  0.006048    0.020161  0.022177  ...  0.004065  0.020325   \n",
       "Anonymous  0.005376  0.013441    0.008065  0.005376  ...  0.005495  0.000000   \n",
       "Anonymous  0.004425  0.002212    0.006637  0.004425  ...  0.009091  0.000000   \n",
       "Anonymous  0.034483  0.006897    0.000000  0.000000  ...  0.000000  0.000000   \n",
       "Anonymous  0.021277  0.000000    0.006079  0.006079  ...  0.006494  0.000000   \n",
       "\n",
       "                 nu       wel       ons     haten      deze       wat  \\\n",
       "index                                                                   \n",
       "Anonymous  0.014706  0.000000  0.000000  0.000000  0.014706  0.000000   \n",
       "Anonymous  0.008065  0.016129  0.004032  0.008065  0.004032  0.004032   \n",
       "Anonymous  0.009009  0.000000  0.000000  0.009009  0.000000  0.009009   \n",
       "Anonymous  0.000000  0.000000  0.000000  0.017857  0.035714  0.000000   \n",
       "Anonymous  0.007752  0.000000  0.007752  0.000000  0.000000  0.000000   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "Anonymous  0.004065  0.024390  0.000000  0.016260  0.008130  0.016260   \n",
       "Anonymous  0.000000  0.027473  0.010989  0.000000  0.005495  0.016484   \n",
       "Anonymous  0.009091  0.000000  0.000000  0.004545  0.000000  0.000000   \n",
       "Anonymous  0.000000  0.000000  0.000000  0.030303  0.000000  0.000000   \n",
       "Anonymous  0.000000  0.051948  0.000000  0.006494  0.000000  0.000000   \n",
       "\n",
       "                dit       bij  \n",
       "index                          \n",
       "Anonymous  0.000000  0.000000  \n",
       "Anonymous  0.000000  0.000000  \n",
       "Anonymous  0.000000  0.000000  \n",
       "Anonymous  0.053571  0.017857  \n",
       "Anonymous  0.000000  0.000000  \n",
       "...             ...       ...  \n",
       "Anonymous  0.008130  0.000000  \n",
       "Anonymous  0.005495  0.005495  \n",
       "Anonymous  0.000000  0.000000  \n",
       "Anonymous  0.000000  0.000000  \n",
       "Anonymous  0.000000  0.006494  \n",
       "\n",
       "[5502 rows x 148 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e80cd49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00fd75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(train, test, leave_one_out=False, dim_reduc=None, norms=True, kernel=\"LinearSVC\", final_pred=False):\n",
    "    \"\"\"\n",
    "    Function to train svm\n",
    "    :param train: train data... (in panda dataframe)\n",
    "    :param test: test data (itou)\n",
    "    :param leave_one_out: whether or not to perform leave-one-out cross validation\n",
    "    :param dim_reduc: dimensionality reduction of input data. Implemented values are pca and som.\n",
    "    :param norms: perform normalisations, i.e. z-scores and L2 (default True)\n",
    "    :param kernel: kernel for SVM\n",
    "    :param final_pred: do the final predictions?\n",
    "    :return: returns a pipeline with a fitted svm model, and if possible prints evaluation and writes to disk:\n",
    "    confusion_matrix.csv, misattributions.csv and (if required) FINAL_PREDICTIONS.csv\n",
    "    \"\"\"\n",
    "\n",
    "    print(\".......... Formatting data ........\")\n",
    "    # Save the classes\n",
    "    classes = list(train.index)\n",
    "    #train = train.drop(['author', 'lang'], axis=1)\n",
    "\n",
    "    if test is not None:\n",
    "        #classes_test = list(test.loc[:, 'author'])\n",
    "        classes_test = list(test.index) ###### !!!!!\n",
    "        #test = test.drop(['author', 'lang'], axis=1)\n",
    "        preds_index = list(test.index)\n",
    "\n",
    "    nfeats = train.columns.__len__()\n",
    "\n",
    "    # CREATING PIPELINE\n",
    "    print(\".......... Creating pipeline according to user choices ........\")\n",
    "    estimators = []\n",
    "\n",
    "    if dim_reduc == 'pca':\n",
    "        print(\".......... using PCA ........\")\n",
    "        estimators.append(('dim_reduc', decomp.PCA()))  # chosen with default\n",
    "        # wich is: n_components = min(n_samples, n_features)\n",
    "\n",
    "#    if dim_reduc == 'som':\n",
    "#        print(\".......... using SOM ........\")  # TODO: fix SOM\n",
    "#        som = minisom.MiniSom(20, 20, nfeats, sigma=0.3, learning_rate=0.5)  # initialization of 50x50 SOM\n",
    "#        # TODO: set robust defaults, and calculate number of columns automatically\n",
    "#        som.train_random(train.values, 100)\n",
    "#        # too long to compute\n",
    "#        # som.quantization_error(train)\n",
    "#        print(\".......... assigning SOM coordinates to texts ........\")\n",
    "#        train = som.quantization(train.values)\n",
    "#        test = som.quantization(test.values)\n",
    "\n",
    "    if norms:\n",
    "        # Z-scores\n",
    "        # TODO: me suis embeté à implémenter quelque chose qui existe\n",
    "        # déjà via sklearn.preprocessing.StandardScaler()\n",
    "        print(\".......... using normalisations ........\")\n",
    "        estimators.append(('scaler', preproc.StandardScaler()))\n",
    "        # scaler = preproc.StandardScaler().fit(train)\n",
    "        # train = scaler.transform(train)\n",
    "        # test = scaler.transform(test)\n",
    "        # feat_stats = pd.DataFrame(columns=[\"mean\", \"std\"])\n",
    "        # feat_stats.loc[:, \"mean\"] = list(train.mean(axis=0))\n",
    "        # feat_stats.loc[:, \"std\"] = list(train.std(axis=0))\n",
    "        # feat_stats.to_csv(\"feat_stats.csv\")\n",
    "        #\n",
    "        # for col in list(train.columns):\n",
    "        #     if not train[col].sum() == 0:\n",
    "        #         train[col] = (train[col] - train[col].mean()) / train[col].std()\n",
    "        #\n",
    "        # for index, col in enumerate(test.columns):\n",
    "        #     if not test.iloc[:, index].sum() == 0:\n",
    "        #         # keep same as train if possible\n",
    "        #         if not feat_stats.loc[index,\"mean\"] == 0 and not feat_stats.loc[index,\"std\"] == 0:\n",
    "        #             test.iloc[:,index] = (test.iloc[:,index] - feat_stats.loc[index,\"mean\"]) / feat_stats.loc[index,\"std\"]\n",
    "        #\n",
    "        #         else:\n",
    "        #             test.iloc[:, index] = (test.iloc[:, index] - test.iloc[:, index].mean()) / test.iloc[:, index].std()\n",
    "\n",
    "        # NB: je ne refais pas la meme erreur, et cette fois j'utilise le built-in\n",
    "        # normalisation L2\n",
    "        # cf. https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Normalizer.html#sklearn.preprocessing.Normalizer\n",
    "\n",
    "        estimators.append(('normalizer', preproc.Normalizer()))\n",
    "        # transformer = preproc.Normalizer().fit(train)\n",
    "        # train = transformer.transform(train)\n",
    "        # transformer = preproc.Normalizer().fit(test)\n",
    "        # test = transformer.transform(test)\n",
    "\n",
    "    print(\".......... choosing SVM ........\")\n",
    "    # let's try a standard one: only with PCA, otherwise too hard\n",
    "    # if withPca:\n",
    "    #    classif = sk.SVC(kernel='linear')\n",
    "\n",
    "    # else:\n",
    "    # try a faster one\n",
    "    #    classif = sk.LinearSVC()\n",
    "\n",
    "    if kernel == \"LinearSVC\":\n",
    "        # try a faster one\n",
    "        estimators.append(('model', sk.LinearSVC()))\n",
    "        # classif = sk.LinearSVC()\n",
    "\n",
    "    else:\n",
    "        estimators.append(('model', sk.SVC(kernel=kernel)))\n",
    "        # classif = sk.SVC(kernel=kernel)\n",
    "\n",
    "    print(\".......... Creating pipeline with steps ........\")\n",
    "    print(estimators)\n",
    "    pipe = skp.Pipeline(estimators)\n",
    "\n",
    "    # Now, doing leave one out validation or training single SVM with train / test split\n",
    "\n",
    "    if leave_one_out:\n",
    "        loo = skmodel.LeaveOneOut()\n",
    "        print(\".......... leave-one-out cross validation will be performed ........\")\n",
    "        print(\".......... using \" + str(loo.get_n_splits(train)) + \" samples ........\")\n",
    "\n",
    "        # Will need to\n",
    "        # 1. train a model\n",
    "        # 2. get prediction\n",
    "        # 3. compute score: precision, recall, F1 for all categories\n",
    "\n",
    "        skmodel.cross_val_score(pipe, train, classes, cv=loo, verbose=1, n_jobs=-1)\n",
    "\n",
    "        # Create the preds array\n",
    "        preds = np.array([], dtype='<U9')\n",
    "        for train_index, test_index in loo.split(train):\n",
    "            # print(test_index)\n",
    "            pipe.fit(train.iloc[train_index, ], [classes[i] for i in list(train_index)])\n",
    "            preds = np.concatenate((preds, pipe.predict(train.iloc[test_index, ])))\n",
    "\n",
    "        # and now, leave one out evaluation (very small redundancy here, one line that could be stored elsewhere)\n",
    "        unique_labels = list(set(classes))\n",
    "        pd.DataFrame(metrics.confusion_matrix(classes, preds, labels=unique_labels),\n",
    "                         index=['true:{:}'.format(x) for x in unique_labels],\n",
    "                         columns=['pred:{:}'.format(x) for x in unique_labels]).to_csv(\"confusion_matrix.csv\")\n",
    "\n",
    "        print(metrics.classification_report(classes, preds))\n",
    "        # writing misattributions\n",
    "        pd.DataFrame([i for i in zip(list(train.index), list(classes), list(preds)) if i[1] != i[2] ],\n",
    "                         columns=[\"id\", \"True\", \"Pred\"]\n",
    "                         ).set_index('id').to_csv(\"misattributions.csv\")\n",
    "\n",
    "        # and now making the model for final preds after leave one out if necessary\n",
    "        if final_pred:\n",
    "            print(\".......... Training final SVM with all train set ........\")\n",
    "            pipe.fit(train, classes)\n",
    "            preds = pipe.predict(test)\n",
    "            #pd.DataFrame(data={'filename': preds_index, 'author': list(preds)}).to_csv(\"FINAL_PREDICTIONS.csv\") # already there lower\n",
    "\n",
    "    # And now the simple case where there is only one svm to train\n",
    "    else:\n",
    "        pipe.fit(train, classes)\n",
    "        preds = pipe.predict(test)\n",
    "        # and evaluate\n",
    "        unique_labels = list(set(classes + classes_test))\n",
    "\n",
    "        pd.DataFrame(metrics.confusion_matrix(classes_test, preds, labels=unique_labels),\n",
    "                         index=['true:{:}'.format(x) for x in unique_labels],\n",
    "                         columns=['pred:{:}'.format(x) for x in unique_labels]).to_csv(\"confusion_matrix.csv\")\n",
    "\n",
    "        print(metrics.classification_report(classes_test, preds))\n",
    "\n",
    "    # AND NOW, we need to evaluate or create the final predictions\n",
    "    if final_pred:\n",
    "        print(\".......... Writing final predictions to FINAL_PREDICTIONS.csv ........\")\n",
    "        # Get the decision function too\n",
    "        myclasses = pipe.classes_\n",
    "        decs = pipe.decision_function(test)\n",
    "        dists = {}\n",
    "        for myclass in enumerate(myclasses):\n",
    "            dists[myclass[1]] = [d[myclass[0]] for d in decs]\n",
    "\n",
    "        pd.DataFrame(data={**{'filename': preds_index, 'author': list(preds)}, **dists}).to_csv(\"FINAL_PREDICTIONS.csv\")\n",
    "\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c85198c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".......... Formatting data ........\n",
      ".......... Creating pipeline according to user choices ........\n",
      ".......... using normalisations ........\n",
      ".......... choosing SVM ........\n",
      ".......... Creating pipeline with steps ........\n",
      "[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('normalizer', Normalizer(copy=True, norm='l2')), ('model', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
      "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "          verbose=0))]\n",
      "                                       precision    recall  f1-score   support\n",
      "\n",
      "                  Albertus Buitendyck       0.00      0.00      0.00         0\n",
      "                            Anonymous       1.00      0.97      0.99      5502\n",
      "                         Claes Stapel       0.00      0.00      0.00         0\n",
      "                       D.V. Coornhert       0.00      0.00      0.00         0\n",
      "                 Guilielmus Bolognino       0.00      0.00      0.00         0\n",
      "                            J. Tradel       0.00      0.00      0.00         0\n",
      "                       Jacob Steendam       0.00      0.00      0.00         0\n",
      "                  Jan van Hoogstraten       0.00      0.00      0.00         0\n",
      "           Joannes Six van Chandelier       0.00      0.00      0.00         0\n",
      "      Joannes Stalpaert van der Wiele       0.00      0.00      0.00         0\n",
      "               Jodocus van Lodenstein       0.00      0.00      0.00         0\n",
      "                      Johan Fruytiers       0.00      0.00      0.00         0\n",
      "          Johannes Wilhelmus Bussingh       0.00      0.00      0.00         0\n",
      "                     Karel van Mander       0.00      0.00      0.00         0\n",
      "                   Lucas van Mechelen       0.00      0.00      0.00         0\n",
      "Philips van Marnix van Sint Aldegonde       0.00      0.00      0.00         0\n",
      "         Pieter Lenaerts van der Goes       0.00      0.00      0.00         0\n",
      "                      Willem de Swaen       0.00      0.00      0.00         0\n",
      "\n",
      "                             accuracy                           0.97      5502\n",
      "                            macro avg       0.06      0.05      0.05      5502\n",
      "                         weighted avg       1.00      0.97      0.99      5502\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/humanum/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "svm = train_svm(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baf77bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
